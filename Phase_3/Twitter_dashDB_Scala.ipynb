{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results = sqlContext.sql(\"SELECT TWEET_LANG, count(1) as totTweets from tweetdata group by TWEET_LANG order by totTweets\")\n",
    "results.cache()\n",
    "results.count()\n",
    "results.collect\n",
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TweetsPerLang.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: path swift://notebooks.spark/Top10UsersWithFollowers.parquet already exists.\n",
       "StackTrace: scala.sys.package$.error(package.scala:27)\n",
       "org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:103)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)\n",
       "org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n",
       "org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)\n",
       "org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:950)\n",
       "org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:950)\n",
       "org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:336)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)\n",
       "org.apache.spark.sql.DataFrame.saveAsParquetFile(DataFrame.scala:1508)\n",
       "$line56.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:19)\n",
       "$line56.$read$$iwC$$iwC$$iwC.<init>(<console>:24)\n",
       "$line56.$read$$iwC$$iwC.<init>(<console>:26)\n",
       "$line56.$read$$iwC.<init>(<console>:28)\n",
       "$line56.$read.<init>(<console>:30)\n",
       "$line56.$read$.<init>(<console>:34)\n",
       "$line56.$read$.<clinit>(<console>)\n",
       "java.lang.J9VMInternals.initializeImpl(Native Method)\n",
       "java.lang.J9VMInternals.initialize(J9VMInternals.java:235)\n",
       "$line56.$eval$.<init>(<console>:7)\n",
       "$line56.$eval$.<clinit>(<console>)\n",
       "java.lang.J9VMInternals.initializeImpl(Native Method)\n",
       "java.lang.J9VMInternals.initialize(J9VMInternals.java:235)\n",
       "$line56.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n",
       "java.lang.reflect.Method.invoke(Method.java:620)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n",
       "java.lang.Thread.run(Thread.java:801)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results = sqlContext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MAX(TWEET_USER_FOLLOWERS_COUNT) AS TWEET_USER_FOLLOWERS_COUNT FROM tweetdata group by TWEET_USER_SCREEN_NAME order by TWEET_USER_FOLLOWERS_COUNT DESC LIMIT 10\")\n",
    "results.cache()\n",
    "results.collect()\n",
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/Top10UsersWithFollowers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val results = sqlContext.sql(\"SELECT TWEET_PLACE_COUNTRY_CODE, COUNT(1) AS TWEET_COUNT FROM tweetdata WHERE TWEET_PLACE_COUNTRY_CODE IS NOT NULL group by TWEET_PLACE_COUNTRY_CODE order by TWEET_COUNT DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TWEET_PLACE_COUNTRY_CODE: string, TWEET_COUNT: bigint]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([US,14], [IN,6], [AR,4], [ES,3], [TH,2], [TR,2], [VE,2], [PH,2], [RU,2], [FR,2], [GB,2], [GH,1], [MY,1], [NL,1], [UA,1], [BB,1], [BR,1], [ID,1], [IT,1], [PE,1], [CO,1], [PY,1], [DE,1], [KE,1], [KR,1], [FI,1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val results = sqlContext.sql(\"SELECT TWEET_CREATED_AT  from tweetdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TWEET_CREATED_AT: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([Wed Nov 25 22:05:34 +0000 2015], [Wed Nov 25 22:05:35 +0000 2015], [Wed Nov 25 22:05:36 +0000 2015], [Wed Nov 25 22:05:36 +0000 2015], [Wed Nov 25 22:05:36 +0000 2015], [Wed Nov 25 22:05:37 +0000 2015], [Wed Nov 25 22:05:37 +0000 2015], [Wed Nov 25 22:05:37 +0000 2015], [Wed Nov 25 22:05:37 +0000 2015], [Wed Nov 25 22:05:37 +0000 2015], [Wed Nov 25 22:05:38 +0000 2015], [Wed Nov 25 22:05:37 +0000 2015], [Wed Nov 25 22:05:38 +0000 2015], [Wed Nov 25 22:05:38 +0000 2015], [Wed Nov 25 22:05:38 +0000 2015], [Wed Nov 25 22:05:39 +0000 2015], [Wed Nov 25 22:05:39 +0000 2015], [Wed Nov 25 22:05:39 +0000 2015], [Wed Nov 25 22:05:39 +0000 2015], [Wed Nov 25 22:05:40 +0000 2015], [Wed Nov 25 22:05:40 +0000 2015], [Wed Nov 25 22:05:40 +0000 2015], [W..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val results = sqlContext.sql(\"SELECT TWEET_CREATED_AT, count(1) as TWEET_COUNT  from tweetdata group by TWEET_CREATED_AT order by TWEET_COUNT DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TWEET_CREATED_AT: string, TWEET_COUNT: bigint]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([Thu Nov 26 07:20:06 +0000 2015,80], [Thu Nov 26 07:45:07 +0000 2015,80], [Thu Nov 26 07:25:04 +0000 2015,74], [Thu Nov 26 20:10:09 +0000 2015,71], [Thu Nov 26 07:10:08 +0000 2015,66], [Thu Nov 26 07:05:04 +0000 2015,57], [Thu Nov 26 07:40:06 +0000 2015,55], [Thu Nov 26 07:40:08 +0000 2015,55], [Thu Nov 26 07:20:07 +0000 2015,55], [Thu Nov 26 07:30:10 +0000 2015,49], [Thu Nov 26 07:45:06 +0000 2015,48], [Thu Nov 26 07:40:07 +0000 2015,43], [Thu Nov 26 07:10:07 +0000 2015,41], [Thu Nov 26 12:34:15 +0000 2015,29], [Thu Nov 26 13:43:11 +0000 2015,29], [Thu Nov 26 07:10:06 +0000 2015,29], [Thu Nov 26 20:10:10 +0000 2015,26], [Thu Nov 26 12:30:05 +0000 2015,26], [Thu Nov 26 14:32:39 +0000 2015,25], [Thu Nov 26 13:44:11 +0000 2015,23], [Thu Nov 2..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TweetsPerSecond.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results = sqlContext.sql(\"SELECT TWEET_PLACE_COUNTRY_CODE, COUNT(1) AS TWEETS_PER_COUNTRY FROM tweetdata where TWEET_PLACE_COUNTRY_CODE IS NOT NULL group by TWEET_PLACE_COUNTRY_CODE order by TWEETS_PER_COUNTRY DESC LIMIT 10\")\n",
    "results.cache()\n",
    "results.count()\n",
    "results.collect\n",
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/Top10CountriesList.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: path swift://notebooks.spark/Top20ActiveUsersList.parquet already exists.\n",
       "StackTrace: scala.sys.package$.error(package.scala:27)\n",
       "org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:103)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)\n",
       "org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n",
       "org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)\n",
       "org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:950)\n",
       "org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:950)\n",
       "org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:336)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)\n",
       "org.apache.spark.sql.DataFrame.saveAsParquetFile(DataFrame.scala:1508)\n",
       "$line82.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:19)\n",
       "$line82.$read$$iwC$$iwC$$iwC.<init>(<console>:24)\n",
       "$line82.$read$$iwC$$iwC.<init>(<console>:26)\n",
       "$line82.$read$$iwC.<init>(<console>:28)\n",
       "$line82.$read.<init>(<console>:30)\n",
       "$line82.$read$.<init>(<console>:34)\n",
       "$line82.$read$.<clinit>(<console>)\n",
       "java.lang.J9VMInternals.initializeImpl(Native Method)\n",
       "java.lang.J9VMInternals.initialize(J9VMInternals.java:235)\n",
       "$line82.$eval$.<init>(<console>:7)\n",
       "$line82.$eval$.<clinit>(<console>)\n",
       "java.lang.J9VMInternals.initializeImpl(Native Method)\n",
       "java.lang.J9VMInternals.initialize(J9VMInternals.java:235)\n",
       "$line82.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n",
       "java.lang.reflect.Method.invoke(Method.java:620)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n",
       "java.lang.Thread.run(Thread.java:801)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results = sqlContext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MAX(TWEET_USER_STATUSES_COUNT) AS TWEET_USER_STATUSES_COUNT FROM tweetdata group by TWEET_USER_SCREEN_NAME order by TWEET_USER_STATUSES_COUNT DESC LIMIT 20\")\n",
    "results.cache()\n",
    "results.count()\n",
    "results.collect\n",
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/Top20ActiveUsersList.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: path swift://notebooks.spark/UserPerYear.parquet already exists.\n",
       "StackTrace: scala.sys.package$.error(package.scala:27)\n",
       "org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:103)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)\n",
       "org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)\n",
       "org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n",
       "org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)\n",
       "org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:950)\n",
       "org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:950)\n",
       "org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:336)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)\n",
       "org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)\n",
       "org.apache.spark.sql.DataFrame.saveAsParquetFile(DataFrame.scala:1508)\n",
       "$line31.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:19)\n",
       "$line31.$read$$iwC$$iwC$$iwC.<init>(<console>:24)\n",
       "$line31.$read$$iwC$$iwC.<init>(<console>:26)\n",
       "$line31.$read$$iwC.<init>(<console>:28)\n",
       "$line31.$read.<init>(<console>:30)\n",
       "$line31.$read$.<init>(<console>:34)\n",
       "$line31.$read$.<clinit>(<console>)\n",
       "java.lang.J9VMInternals.initializeImpl(Native Method)\n",
       "java.lang.J9VMInternals.initialize(J9VMInternals.java:235)\n",
       "$line31.$eval$.<init>(<console>:7)\n",
       "$line31.$eval$.<clinit>(<console>)\n",
       "java.lang.J9VMInternals.initializeImpl(Native Method)\n",
       "java.lang.J9VMInternals.initialize(J9VMInternals.java:235)\n",
       "$line31.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n",
       "java.lang.reflect.Method.invoke(Method.java:620)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\n",
       "com.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n",
       "java.lang.Thread.run(Thread.java:801)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results = sqlContext.sql(\"SELECT t1.YR as YR, count(1) as CNT FROM (select DISTINCT TWEET_USER_ID, substring(TWEET_USER_CREATED_AT,26) AS YR FROM tweetdata where TWEET_USER_CREATED_AT IS NOT NULL) t1 group by YR order by YR\")\n",
    "results.cache()\n",
    "results.count()\n",
    "results.collect\n",
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/UserPerYear.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([270186,zaibatsu], [185559,spencerrayner], [142008,Bensonix], [140982,SwiftKey], [117400,MikeWellsAuthor], [87984,ActionComplete], [84185,Publicidad_VE], [71720,paul_cude], [71345,MovistarEC], [67601,Bizitweet], [66123,IcebergGem], [65517,daum70000], [63911,TechJobs_NYC], [61856,GovsInfo], [55473,movistarpro_es], [53503,chicabinaria], [53003,SEOCHECKOUT], [50175,williamscrespo3], [48027,Popmidia], [47067,jamesbryronlove])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results = sqlContext.sql(\"SELECT MAX(TWEET_USER_FRIENDS_COUNT) AS FRIENDS_COUNT, TWEET_USER_SCREEN_NAME FROM tweetdata GROUP BY TWEET_USER_SCREEN_NAME ORDER BY FRIENDS_COUNT DESC LIMIT 20\");\n",
    "results.cache()\n",
    "results.count()\n",
    "results.collect\n",
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TopUsersWithFriends1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TopUsersWithFriends1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val results = sqlcontext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MIN(TWEET_USER_FOLLOWERS_COUNT) AS TWEET_USER_FOLLOWERS_COUNT FROM tweetdata group by TWEET_USER_SCREEN_NAME order by TWEET_USER_FOLLOWERS_COUNT DESC LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dashdataDF.registerTempTable(\"tweetdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([NTelevisa_com,2053044], [SexenioMX,1808910], [androides,971855], [AIerta,727727], [zaibatsu,538765], [getitupforme,507139], [liderendeportes,498886], [Geloryo,491668], [CommuterLine,383636], [MovistarEC,352844])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.cache()\n",
    "results.count()\n",
    "results.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ServiceEbooks,-1], [GMDFirefox,1], [ngocphong195,1], [banyufo,1], [jZMJMPfYtcmAJv6,1], [GNjuG3L7dDR46jS,1], [Ericamaciel19,1], [sproypef,1], [Debrari80045521,1], [9Bozhenka,1], [kasady_cletus,1], [plapped_team,1], [JamesEarl921,1], [WendyTh1977,1], [tonystar77,1], [RenKrger3,1], [oyfCUNw5khg3IZh,1], [ngocdie16683486,1], [lSnDv9nQAoAQPbC,1], [Catalinasimona6,1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = sqlcontext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MAX(TWEET_USER_STATUSES_COUNT) AS TWEET_USER_STATUSES_COUNT FROM tweetdata group by TWEET_USER_SCREEN_NAME order by TWEET_USER_STATUSES_COUNT\")\n",
    "results.cache()\n",
    "results.count()\n",
    "results.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TopUsersWithFriends5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val results = sqlcontext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MAX(TWEET_USER_FOLLOWERS_COUNT) AS TWEET_USER_FOLLOWERS_COUNT FROM tweetdata group by TWEET_USER_SCREEN_NAME order by TWEET_USER_FOLLOWERS_COUNT LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ServiceEbooks,-1], [CtaCgbyME5x2K2k,0], [porchung,0], [kGjpjnfXQWfFbzF,0], [GigelPacala,0], [inna_innag987,0], [22109906bdfd45f,0], [lSnDv9nQAoAQPbC,0], [kkchu22,0], [NichollsJuliana,0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.cache()\n",
    "results.count()\n",
    "results.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TopUsersWithFriends4.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results = sqlContext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MAX(TWEET_USER_FOLLOWERS_COUNT) AS TWEET_USER_FOLLOWERS_COUNT, TWEET_PLACE_COUNTRY_CODE FROM tweetdata where TWEET_PLACE_COUNTRY_CODE IS NOT NULL group by TWEET_USER_SCREEN_NAME, TWEET_PLACE_COUNTRY_CODE order by TWEET_USER_FOLLOWERS_COUNT DESC LIMIT 10\")\n",
    "results.cache()\n",
    "results.count()\n",
    "results.collect\n",
    "results.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TopUsersPerCountry.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val dashdataDF = sqlContext.load(\"jdbc\", Map(\n",
    "\"url\" -> \"jdbc:db2://bluemix05.bluforcloud.com:50000/BLUDB:user=dash019411;password=2k3n2TR3x3mb;\",\n",
    "\"dbtable\" -> \"DASH019411.NORED\"))\n",
    "dashdataDF.registerTempTable(\"tweetdata\")\n",
    "sqlContext.cacheTable(\"tweetdata\")\n",
    "val results1 = sqlContext.sql(\"SELECT TWEET_LANG, count(1) as totTweets from tweetdata group by TWEET_LANG order by totTweets\")\n",
    "results1.cache()\n",
    "results1.count()\n",
    "results1.collect\n",
    "results1.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TweetsPerLang.parquet\")\n",
    "val results2 = sqlContext.sql(\"SELECT MAX(TWEET_USER_FRIENDS_COUNT) AS FRIENDS_COUNT, TWEET_USER_SCREEN_NAME FROM tweetdata GROUP BY TWEET_USER_SCREEN_NAME ORDER BY FRIENDS_COUNT DESC LIMIT 20\");\n",
    "results2.cache()\n",
    "results2.count()\n",
    "results2.collect\n",
    "results2.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/TopUsersWithFriends.parquet\")\n",
    "val results3 = sqlContext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MAX(TWEET_USER_FOLLOWERS_COUNT) AS TWEET_USER_FOLLOWERS_COUNT FROM tweetdata group by TWEET_USER_SCREEN_NAME order by TWEET_USER_FOLLOWERS_COUNT DESC LIMIT 10\")\n",
    "results3.cache()\n",
    "results3.collect()\n",
    "results3.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/Top10UsersWithFollowers.parquet\")\n",
    "val results4 = sqlContext.sql(\"SELECT TWEET_PLACE_COUNTRY_CODE, COUNT(1) AS TWEETS_PER_COUNTRY FROM tweetdata where TWEET_PLACE_COUNTRY_CODE IS NOT NULL group by TWEET_PLACE_COUNTRY_CODE order by TWEETS_PER_COUNTRY DESC LIMIT 10\")\n",
    "results4.cache()\n",
    "results4.count()\n",
    "results4.collect\n",
    "results4.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/Top10CountriesList.parquet\")\n",
    "val results5 = sqlContext.sql(\"SELECT TWEET_USER_SCREEN_NAME, MAX(TWEET_USER_STATUSES_COUNT) AS TWEET_USER_STATUSES_COUNT FROM tweetdata group by TWEET_USER_SCREEN_NAME order by TWEET_USER_STATUSES_COUNT DESC LIMIT 20\")\n",
    "results5.cache()\n",
    "results5.count()\n",
    "results5.collect\n",
    "results5.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/Top20ActiveUsersList.parquet\")\n",
    "val results6 = sqlContext.sql(\"SELECT t1.YR as YR, count(1) as CNT FROM (select DISTINCT TWEET_USER_ID, substring(TWEET_USER_CREATED_AT,26) AS YR FROM tweetdata where TWEET_USER_CREATED_AT IS NOT NULL) t1 group by YR order by YR\")\n",
    "results6.cache()\n",
    "results6.count()\n",
    "results6.collect\n",
    "results6.repartition(1).saveAsParquetFile(\"swift://notebooks.spark/UserPerYear.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}