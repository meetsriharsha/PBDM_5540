# Import SQLContext and data types
#from pyspark.sql import SQLContext
#from pyspark.sql.types import *

   # conf = new SparkConf().setAppName("TweetCount")
   # conf.setMaster("local")
   # sc = new SparkContext(conf)
    
    # sc is an existing SparkContext.
   # sqlContext = SQLContext(sc)
    #tweet = sqlContext.load("D:/UMKC/Docs/Subjects/PBDM/PB_Project/TweetsPerLang/part-00000", "json")
    #tweet.printSchema()
#text_file = open("D:/UMKC/Docs/Subjects/PBDM/PB_Project/TweetsPerTimeLang/part-00000","r")
#lines = text_file.read().split(',')
#print "\n".join(lines[0])
#print len(lines)
#text_file.close()
import re

lang = []
count = []
f = open( 'D:/UMKC/Docs/Subjects/PBDM/PB_Project/TweetsPerLang/part-00000', 'rU' ) #open the file in read universal mode
for line in f:
    cells = line.split( "," )
    lang.append( ( re.sub('[\(\)\"]','',cells[ 0 ] ) )) #since we want the first, second and third column
    count.append( ( re.sub('[\(\)\"]','',cells[ 1 ] ) ))

f.close()
print lang
print "\n".join(lang,count)
print count
print "\n".join(count)
